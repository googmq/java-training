本节示例代码请见[文件](res/demo.py)，基于tensorflow

# 线性回归的概念

## 输入（特征、Feature）

X1，X2，X3。。。Xn

## 输出（结果、Result）

Y

## 模型（Model）

Y=B0+B1✖X1+B2✖X2。。。Bn✖Xn

![](res\２.png)

## 定义

输出可以使用输入的线性组合解析式进行表达。

![](res\１.jpg)

# 问题思路

模型针对输入的计算结果与实际输出存在偏差：bias=Yn-Y，每一组输入与输出都有自己的偏差。

所有组数据偏差的平方相加之和，成为损失函数。

求一个最接近解，使得损失函数取值最小。

## 损失函数

![](res\３.png)

# 最小二乘法

是一个直接的数学求解公式，不过它要求X是列满秩的

![](res\４.png)

# 梯度下降法

## 微分

- 函数图像中，某点的切线的斜率
- 函数的变化率

## 梯度

当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分。

梯度实际上就是多变量微分的一般化。

![](res\７.png)

我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用<>包括起来，说明梯度其实一个向量。

## 梯度下降法公式

![](res\６.png)

- α是什么含义？
  α在梯度下降算法中被称作为**学习率**或者**步长**，意味着我们可以通过α来控制每一步走的距离，是算法的一个参数。

  ![](res\８.png)

- 为什么要梯度要乘以一个负号？
  梯度前加一个负号，就意味着朝着梯度相反的方向前进

# 单变量函数的梯度下降

假设有一个单变量的函数
![img](res\9.png)

函数的微分

![img](res\10.png)

 初始化，起点为

![img](res\11.png)

学习率为

![img](res\12.png)

 进行梯度下降的迭代计算过程：

![img](res\13.png)

 如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底

![img](res\14.png)

## 多变量函数的梯度下降

假设有一个目标函数

![img](res\15.png)
假设初始的起点为

![img](res\16.png)

 初始的学习率为：

![img](res\17.png)

函数的梯度为：

![img](res\18.png)

 进行多次迭代：

![img](res\19.png)

 已经基本靠近函数的最小值点

![img](res\20.png)
